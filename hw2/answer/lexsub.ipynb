{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lexsub:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrofitting Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy\n",
    "import re\n",
    "import sys\n",
    "from pymagnitude import *\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def print_output(word_vecs, outFile):\n",
    "    w_file = open(outFile,'w')\n",
    "    for word in wordvecs:\n",
    "        w_file.write(word)\n",
    "        for vec in word_vecs[word]:\n",
    "            w_file.write(' '+str(vec))\n",
    "        w_file.write('\\n')\n",
    "    w_file.close()\n",
    "\n",
    "def read_wordvecs(filename):\n",
    "    wordvecs = {}\n",
    "    wv = Magnitude(filename)\n",
    "    \n",
    "    for word in wv:\n",
    "        wordvecs[word[0]] = word[1]\n",
    "    return wordvecs \n",
    "\n",
    "def lex_dict(filename):\n",
    "        lexicon = {}\n",
    "        for line in open(filename,'r'):\n",
    "            word_list = line.lower().strip().split()\n",
    "            lexicon[word_list[0]] = [word for word in word_list[1:]]\n",
    "        return lexicon\n",
    "\n",
    "\n",
    "def retrofit(wordvecs, lexicon, iter):\n",
    "    new_vecs = deepcopy(wordvecs)\n",
    "    vocab = set(new_vecs.keys())\n",
    "    common_vocab = vocab.intersection(set(lexicon.keys()))\n",
    "    for i in range(iter):\n",
    "        for word in common_vocab:\n",
    "            w_neighbours = set(lexicon[word]).intersection(vocab)\n",
    "            n_neighbours = len(w_neighbours)\n",
    "            if n_neighbours > 0:\n",
    "                new_vector = n_neighbours * wordvecs[word]\n",
    "                for pword in w_neighbours:\n",
    "                    new_vector = new_vector + new_vecs[pword]\n",
    "                new_vecs[word] = new_vector/(2*n_neighbours)\n",
    "    return new_vecs\n",
    "\n",
    "\n",
    "    \n",
    "wordvecs = read_wordvecs('data/glove.6B.100d.magnitude')\n",
    "lexicon = lex_dict('data/lexicons/wordnet-synonyms.txt')\n",
    "print_output(retrofit(wordvecs,lexicon,10),'data/glove.6B.100d.retrofit.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lexsub import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the solution on dev (with retrofitted embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "english edge line position place front back while way point\nenglish edge line position place front back while way point\nenglish edge line position place front back while way point\nenglish edge line position place front back while way point\nenglish edge line position place front back while way point\nenglish edge line position place front back while way point\nenglish edge line position place front back while way point\nenglish edge line position place front back while way point\nenglish edge line position place front back while way point\nenglish edge line position place front back while way point\n"
    }
   ],
   "source": [
    "lexsub = LexSub(os.path.join('../data','glove.6B.100d.retrofit.magnitude'))\n",
    "output = []\n",
    "with open(os.path.join('../data','input','dev.txt')) as f:\n",
    "    for line in f:\n",
    "        fields = line.strip().split('\\t')\n",
    "        output.append(\" \".join(lexsub.substitutes(int(fields[0].strip()), fields[1].strip().split())))\n",
    "print(\"\\n\".join(output[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Score=46.56\n"
    }
   ],
   "source": [
    "from lexsub_check import precision\n",
    "with open(os.path.join('../data','reference','dev.out'), 'rt') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "print(\"Score={:.2f}\".format(100*precision(ref_data, output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "Write some beautiful documentation of your program here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Do some analysis of the results. What ideas did you try? What worked and what did not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lexicon file difference in output\n",
    "Comparing all the different lexicon files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "place both back bottom away sides onto front edge line\n",
      "both place along away back sides onto line bottom front\n",
      "both place along away back line sides onto edge front\n",
      "both place away back along onto bottom line sides front\n",
      "both place along away back onto sides line bottom edge\n",
      "back both onto away edge line place bottom front along\n",
      "both place along back sides away line onto bottom front\n",
      "both place along away back onto sides line bottom edge\n",
      "both place along away back onto sides line bottom edge\n",
      "along both place away line onto sides edge back front\n"
     ]
    }
   ],
   "source": [
    "lexsub = LexSub(os.path.join('data','glove.6B.100d.retrofit-ppdb.magnitude'))\n",
    "output = []\n",
    "with open(os.path.join('data','input','dev.txt')) as f:\n",
    "    for line in f:\n",
    "        fields = line.strip().split('\\t')\n",
    "        output.append(\" \".join(lexsub.substitutes(int(fields[0].strip()), fields[1].strip().split())))\n",
    "print(\"\\n\".join(output[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score=44.69\n"
     ]
    }
   ],
   "source": [
    "from lexsub_check import precision\n",
    "with open(os.path.join('data','reference','dev.out'), 'rt') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "print(\"Score={:.2f}\".format(100*precision(ref_data, output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english place while back point edge way line position along\n",
      "way place while position back english along point line front\n",
      "english while way point place along line position edge back\n",
      "way while place back english line position point along front\n",
      "while place along way english point line position back front\n",
      "while way back edge line point place english position along\n",
      "place way while english position line point along back front\n",
      "while place along way english point line position back front\n",
      "while place along way english point line position back front\n",
      "along edge english line point way place while back position\n"
     ]
    }
   ],
   "source": [
    "lexsub = LexSub(os.path.join('data','glove.6B.100d.retrofit-wordnet.magnitude'))\n",
    "output = []\n",
    "with open(os.path.join('data','input','dev.txt')) as f:\n",
    "    for line in f:\n",
    "        fields = line.strip().split('\\t')\n",
    "        output.append(\" \".join(lexsub.substitutes(int(fields[0].strip()), fields[1].strip().split())))\n",
    "print(\"\\n\".join(output[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score=45.51\n"
     ]
    }
   ],
   "source": [
    "from lexsub_check import precision\n",
    "with open(os.path.join('data','reference','dev.out'), 'rt') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "print(\"Score={:.2f}\".format(100*precision(ref_data, output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "then back place bottom left way corner edge line front\n",
      "way then back place along line front bottom left edge\n",
      "way then along place line edge back left front corner\n",
      "way then back place line bottom front along left edge\n",
      "along then way place left back line front edge bottom\n",
      "then way back edge left line front along place bottom\n",
      "then place way line back along front left bottom edge\n",
      "along then way place left back line front edge bottom\n",
      "along then way place left back line front edge bottom\n",
      "along line edge way then left place back front corner\n"
     ]
    }
   ],
   "source": [
    "lexsub = LexSub(os.path.join('data','glove.6B.100d.retrofit-wordnet+.magnitude'))\n",
    "output = []\n",
    "with open(os.path.join('data','input','dev.txt')) as f:\n",
    "    for line in f:\n",
    "        fields = line.strip().split('\\t')\n",
    "        output.append(\" \".join(lexsub.substitutes(int(fields[0].strip()), fields[1].strip().split())))\n",
    "print(\"\\n\".join(output[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score=40.22\n"
     ]
    }
   ],
   "source": [
    "from lexsub_check import precision\n",
    "with open(os.path.join('data','reference','dev.out'), 'rt') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "print(\"Score={:.2f}\".format(100*precision(ref_data, output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining the two best lexicon files produced less "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "place both back bottom away sides onto front edge line\n",
      "both place along away back sides onto line bottom front\n",
      "both place along away back line sides onto edge front\n",
      "both place away back along onto bottom line sides front\n",
      "both place along away back onto sides line bottom edge\n",
      "back both onto away edge line place bottom front along\n",
      "both place along back sides away line onto bottom front\n",
      "both place along away back onto sides line bottom edge\n",
      "both place along away back onto sides line bottom edge\n",
      "along both place away line onto sides edge back front\n"
     ]
    }
   ],
   "source": [
    "lexsub = LexSub(os.path.join('data','glove.6B.100d.retrofit-ppdb-wordnet.magnitude'))\n",
    "output = []\n",
    "with open(os.path.join('data','input','dev.txt')) as f:\n",
    "    for line in f:\n",
    "        fields = line.strip().split('\\t')\n",
    "        output.append(\" \".join(lexsub.substitutes(int(fields[0].strip()), fields[1].strip().split())))\n",
    "print(\"\\n\".join(output[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score=44.63\n"
     ]
    }
   ],
   "source": [
    "from lexsub_check import precision\n",
    "with open(os.path.join('data','reference','dev.out'), 'rt') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "print(\"Score={:.2f}\".format(100*precision(ref_data, output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempts to incorporate context words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "for the following tests the ppdb-xl lexicon, and multiplicative combination, balMult (from paper mentioned) are used:\n",
    "<ol>\n",
    "\n",
    "<li> Candidates are the 20 most simaler words to target, baseline (10 candidates): 46.4474%</li>\n",
    "</br>\n",
    "<li>Consider set number of words around target (varied from 1 to full sentence) as context: ~30%</li>\n",
    "</br>\n",
    "<li>Consider all words in sentence, selects words above a threshold as context. \n",
    "As the threshold reaches 1, performance reaches baseline, else is lower than baseline.\n",
    "</li>\n",
    "</br>\n",
    "<li>Consider set number of words around target (varied from 1 to full sentence) as context, score the candidates with each context word seperatly and keep best score per word: 39.45%</li>\n",
    "\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempts using multiple lexicons \n",
    "for the following tests, the ppdb-xl lexicon as aware (retrofitted) lexicon and the orignal lexicon as basic lexicon, and balMult are used. Candidates are the 20 most similar words to target\n",
    "\n",
    "<ol>\n",
    "<li>\n",
    "Candidates: aware </br>\n",
    "balMut: aware </br>\n",
    "score: 37%\n",
    "</li>\n",
    "</br>\n",
    "<li>\n",
    "Candidates: aware </br> \n",
    "balMut: basic </br>\n",
    "score: 32% \n",
    "</li>\n",
    "</br>\n",
    "<li>\n",
    "Candidates: basic </br>\n",
    "balMut: aware </br>\n",
    "score: 21% \n",
    "</li>\n",
    "</br>\n",
    "</br>\n",
    "<li>\n",
    "Candidates: aware </br>\n",
    "balMut: (context: aware, target:basic) </br>\n",
    "score: 34% \n",
    "</li>\n",
    "</br>\n",
    "<li>\n",
    "Candidates: aware </br>\n",
    "balMut: (context: basic, target:aware) </br>\n",
    "score: 39% "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python_defaultSpec_1602741625700"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}