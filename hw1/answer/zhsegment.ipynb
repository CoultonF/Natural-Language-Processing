{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# zhsegment: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zhsegment import *\n",
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the baseline (only on unigram counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "中 美 在 沪 签订 高 科技 合作 协议\n新华社 上海 八月 三十一日 电 （ 记者 白 国 良 、 夏儒阁 ）\n“ 中 美 合作 高 科技 项目 签字 仪式 ” 今天 在 上海 举行 。\n"
    }
   ],
   "source": [
    "Pw = Pdist(data=datafile(\"data/count_1w.txt\"), missingfn=penalize_long_unknown_baseline)\n",
    "\n",
    "\n",
    "segmenter = Segment(Pw)\n",
    "output_full = []\n",
    "with open(\"data/input/dev.txt\") as f: \n",
    "    for line in f:\n",
    "        output = \" \".join(segmenter.segment(line.strip()))\n",
    "        output_full.append(output)\n",
    "\n",
    "\n",
    "print(\"\\n\".join(output_full[:3])) # print out the first three lines of output as a sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some details on the baseline\n",
    "\n",
    "The baseline model has been implemented as in the pseudo-code. For unknown words, we realized that the penalty had to be greater because the minimum unigram probability was very low and not much greater from 1/N. Therefore, we used a penalty of:\n",
    "(1/N) * 1000000**-(len(key)-1)\n",
    "\n",
    "This caused some issues in calculating log10 when the unknown word was very long. Whenever this issue was encountered, we simply replace the prbability with 1e-300 i.e: we calculate log10(1e-300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking output for baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "score: 0.86\n"
    }
   ],
   "source": [
    "from zhsegment_check import fscore\n",
    "with open('data/reference/dev.out', 'r') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "    tally = fscore(ref_data, output_full)\n",
    "    print(\"score: {:.2f}\".format(tally), file=sys.stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "中 美 在 沪 签订 高 科技 合作 协议\n新华社 上海 八月 三十一日 电 （ 记者 白 国 良 、 夏儒阁 ）\n“ 中 美 合作 高 科技 项目 签字 仪式 ” 今天 在 上海 举行 。\n"
    }
   ],
   "source": [
    "Pw = Pdist(data=datafile(\"data/count_1w.txt\"), missingfn=penalize_long_unknown) \n",
    "P2w = Pdist(data=datafile(\"data/count_2w.txt\"), missingfn=penalize_long_unknown)\n",
    "\n",
    "\n",
    "segmenter2 = Segment(Pw,P2w)\n",
    "output_bigram = []\n",
    "with open(\"data/input/dev.txt\") as f: \n",
    "    for line in f:\n",
    "        output = \" \".join(segmenter2.segment2(line.strip()))\n",
    "        output_bigram.append(output)\n",
    "print(\"\\n\".join(output_full[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking output for bigram model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "score: 0.93\n"
    }
   ],
   "source": [
    "from zhsegment_check import fscore\n",
    "with open('data/reference/dev.out', 'r') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "    tally = fscore(ref_data, output_bigram)\n",
    "    print(\"score: {:.2f}\".format(tally),file=sys.stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Do some analysis of the results. What ideas did you try? What worked and what did not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penalty for long unknown words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We iteratively search for the correct length based penality function for unknown words.\n",
    "* We find (1/N) * 12500**-(len(key)-1) maximizes our dev score\n",
    "* The results below are done after implementing linearly interpolated bigrams.\n",
    "* Compared to english (hw0) a larger penalty works better. This indicates the average chinese word has less characters than the average english word. A quick google search seems to agree with this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def penalize_long_unknown(key, N): \n",
    "    # return (1/N) * 100**-(len(key)-1) #.79\n",
    "    # return (1/N) * 1000000**-(len(key)-1) #.87\n",
    "    # return (1/N) * 100000**-(len(key)-1) #.92\n",
    "    # return (1/N) * 10000**-(len(key)-1) #.9345\n",
    "    # return (1/N) * 12500**-(len(key)-1) #.9349 <-- Selected\n",
    "    # return (1/N) * 1000**-(len(key)-1) #.92\n",
    "    # return (1/N) * 5000**-(len(key)-1) #.93\n",
    "    # return (1/N) * 7500**-(len(key)-1) #.9334\n",
    "    # return (1/N) * 15000**-(len(key)-1) #.9341"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What ideas worked:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We tried to modify the lambda values for JM by iteratively update the parameters using a step size of 0.05. We found that this parameter was not particularly sensitive, but the maximum parameter value was 0.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lambda values\n",
    "# JM = .7 #.0.9338\n",
    "# JM = .8 #.0.9349\n",
    "# JM = .75 #.0.9337\n",
    "# JM = .9 #.0.9323"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  What ideas didn't work:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also experimented with the replacement of numerical characters with  flags in the distribution. We found that the score would decrease very slightly by ~1 point and decided to not implement this technique as a result. The updating of numerical characters was done using the following code in Pdist as an experimental purpose, as runtime was not a consideration when just testing out the functionality and resultant score. The numerical characters in the runtime were not standard numerical characters as well, which is why they had to be mapped out individually. The text numbers had UTF8 values like \\xEF\\xBC\\x91 which represented the number 1 rather than the typical \\x31."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " #class Pdist(dict):\n",
    "#    \"A probability distribution estimated from counts in datafile.\"\n",
    "#    def __init__(self, data=[], N=None, missingfn=None):\n",
    "#        if re.search('[１２３４５６７８９０]', key):\n",
    "#            key = key.replace('１','<NUM>')\n",
    "#            key = key.replace('２','<NUM>')\n",
    "#            key = key.replace('３','<NUM>')\n",
    "#            key = key.replace('４','<NUM>')\n",
    "#            key = key.replace('５','<NUM>')\n",
    "#            key = key.replace('６','<NUM>')\n",
    "#            key = key.replace('７','<NUM>')\n",
    "#            key = key.replace('８','<NUM>')\n",
    "#            key = key.replace('９','<NUM>')\n",
    "#             key = key.replace('０','<NUM>')\n",
    "#         for key,count in data:\n",
    "#             self[key] = self.get(key, 0) + int(count)\n",
    "#         self.N = float(N or sum(self.values()))\n",
    "#         self.missingfn = missingfn or (lambda k, N: 1./N)\n",
    "#     def __call__(self, key): \n",
    "#         if re.search('[１２３４５６７８９０]', key):\n",
    "#             key = key.replace('１','<NUM>')\n",
    "#             key = key.replace('２','<NUM>')\n",
    "#             key = key.replace('３','<NUM>')\n",
    "#             key = key.replace('４','<NUM>')\n",
    "#             key = key.replace('５','<NUM>')\n",
    "#             key = key.replace('６','<NUM>')\n",
    "#             key = key.replace('７','<NUM>')\n",
    "#             key = key.replace('８','<NUM>')\n",
    "#             key = key.replace('９','<NUM>')\n",
    "#             key = key.replace('０','<NUM>')\n",
    "#         if key in self: return self[key]/self.N  \n",
    "#         else: return self.missingfn(key, self.N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python_defaultSpec_1601521257227"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}