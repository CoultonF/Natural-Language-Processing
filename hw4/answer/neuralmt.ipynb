{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# neuralmt: default program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmt import *\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the default solution on dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq(build=False)\n",
    "model.load(os.path.join('data', 'seq2seq_E049.pt'))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "# loading test dataset\n",
    "test_iter = loadTestData(os.path.join('data', 'input', 'dev.txt'), model.fields['src'],\n",
    "                            device=device, linesToLoad=sys.maxsize)\n",
    "results = translate(model, test_iter) # Warning: will take >5mins depending on your machine\n",
    "print(\"\\n\".join(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the default output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bleu_check import bleu\n",
    "ref_t = []\n",
    "with open(os.path.join('data','reference','dev.out')) as r:\n",
    "    ref_t = r.read().strip().splitlines()\n",
    "print(bleu(ref_t, results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "### Copy Mechanism\n",
    "Inside the translate(model, test_itr) function, we created a simple copy mechanism that will take the unknown word position of the source sentence and insert the source word rather than the \\<unk\\> token. It works with multi-unknown sentences as each unknown word is assigned and index value for the respective word in the sentence source.\n",
    "\n",
    "To get the source unknown reference word a permute of the attention matrix was needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Do some analysis of the results. What ideas did you try? What worked and what did not?\n",
    "\n",
    "\n",
    "### Copy Mechanism\n",
    "\n",
    "Since English and German are both germanic languages, the copy mechanism that we implemented, while not truly perfect, still improves the score a fair amount due to the language similarity.\n",
    "\n",
    "The baseline model had a score of 17.11, while the baseline w/ copy produced a score of 17.40, nearly a .30 point increase of BLEU score.\n",
    "\n",
    "The reason why the model slightly increases the score is due to words OOV having an identical translation, so putting in the corresponding word as a replacement for unknown tokens worked. It is better to use a method of translating the OOV words through fine-tuning and this approach would be more necessary if the languages were not similar.\n",
    "\n",
    "We plotted the attention graph below to get an idea of how the attention mechanism works and aid in adjusting the shapes of tensors to get the original word the is considered OOV. For this example, the word psychotherapie-patientin is the \\<unk\\> word, and the reader can mostly understand the word in English due to both languages being so similar. In terms of the BLEU score, only words with a direct translation will have an improvement on BLEU score. Words such as kebab have an indentical translation in german and english for example.\n",
    "\n",
    "<img src=\"attentionplot.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Beamish search\"\n",
    "# Takes the 2k best outputs and creates k best possible outputs.\n",
    "# Starting with the first words of the k best orignal outputs, we use a trigram probibity\n",
    "# to select the next word. In the example below 'K' and 'k' repersent the two final outputs \n",
    "# and the gird repersents 2k the orignal outputs.\n",
    "\n",
    "# --|--|--|--|--|--|--|--|--|--|--|--|--|--|--|--||--|\n",
    "# K |  |K |k |K |K |K |K |K |K |  |  |  |  |  |  ||  |\n",
    "# --|--|--|--|--|--|--|--|--|--|--|--|--|--|--|--||--|\n",
    "# k |Kk|  |  |k |k |k |k |k |k |Kk|Kk|Kk|Kk|Kk|Kk|\n",
    "# --|--|--|--|--|--|--|--|--|--|--|--|--|--|--|--||--||--|\n",
    "#   |  |k |k |  |  |  |  |  |  |  |  |  |  |  |  ||  ||  |\n",
    "# --|--|--|--|--|--|--|--|--|--|--|--|--|--|--|--||--||--|\n",
    "#   |  |  |K |  |  |  |  |  |  |  |  |  |  |  |  |\n",
    "# --|--|--|--|--|--|--|--|--|--|--|--|--|--|--|--|\n",
    " \n",
    "# The problem with this method was; extremly common words such as \"I\" \"we\" \"was\" \"the\" keep getting selected,\n",
    "# creating very poor sentences. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}